{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCstEfI-I95A"
      },
      "outputs": [],
      "source": [
        "# !pip install fastapi nest-asyncio pyngrok uvicorn torch pillow pydantic transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbPJWA9CRkKu"
      },
      "outputs": [],
      "source": [
        "from contextlib import asynccontextmanager\n",
        "from typing import Optional\n",
        "\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "import torch\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from PIL import Image\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "from torch import nn\n",
        "from transformers import SiglipVisionModel\n",
        "from transformers.modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n",
        "from transformers.models.siglip.configuration_siglip import SiglipVisionConfig\n",
        "from transformers.models.siglip.modeling_siglip import (\n",
        "    SiglipEncoder,\n",
        "    SiglipMultiheadAttentionPoolingHead,\n",
        "    SiglipVisionModel,\n",
        ")\n",
        "from transformers.utils import auto_docstring, can_return_tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SiglipVisionModelNoEmbeddings(SiglipVisionModel):\n",
        "    config_class = SiglipVisionConfig\n",
        "    main_input_name = \"pixel_values\"\n",
        "\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.vision_model = SiglipVisionTransformer(config)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    @can_return_tuple\n",
        "    @auto_docstring\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        interpolate_pos_encoding: bool = False,\n",
        "    ) -> BaseModelOutputWithPooling:\n",
        "        return self.vision_model(\n",
        "            hidden_states=hidden_states,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            interpolate_pos_encoding=interpolate_pos_encoding,\n",
        "        )\n",
        "\n",
        "\n",
        "class SiglipVisionTransformer(nn.Module):\n",
        "    def __init__(self, config: SiglipVisionConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        embed_dim = config.hidden_size\n",
        "\n",
        "        self.encoder = SiglipEncoder(config)\n",
        "        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n",
        "        self.use_head = (\n",
        "            True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n",
        "        )\n",
        "        if self.use_head:\n",
        "            self.head = SiglipMultiheadAttentionPoolingHead(config)\n",
        "\n",
        "    @can_return_tuple\n",
        "    @auto_docstring\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        interpolate_pos_encoding: Optional[bool] = False,\n",
        "    ) -> BaseModelOutputWithPooling:\n",
        "        output_attentions = (\n",
        "            output_attentions\n",
        "            if output_attentions is not None\n",
        "            else self.config.output_attentions\n",
        "        )\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states\n",
        "            if output_hidden_states is not None\n",
        "            else self.config.output_hidden_states\n",
        "        )\n",
        "\n",
        "        encoder_outputs: BaseModelOutput = self.encoder(\n",
        "            inputs_embeds=hidden_states,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "        )\n",
        "\n",
        "        last_hidden_state = encoder_outputs.last_hidden_state\n",
        "        last_hidden_state = self.post_layernorm(last_hidden_state)\n",
        "\n",
        "        pooler_output = self.head(last_hidden_state) if self.use_head else None\n",
        "\n",
        "        return BaseModelOutputWithPooling(\n",
        "            last_hidden_state=last_hidden_state,\n",
        "            pooler_output=pooler_output,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "def load_vit(path_vit_pt: str):\n",
        "    vision_config = SiglipVisionConfig(name_or_path=\"google/siglip-base-patch16-224\")\n",
        "    model = SiglipVisionModelNoEmbeddings(vision_config)\n",
        "\n",
        "    vit_weights = torch.load(path_vit_pt)\n",
        "    encoder_weights = vit_weights[\"encoder\"]\n",
        "    head_weights = vit_weights[\"head\"]\n",
        "    post_layernorm_weights = vit_weights[\"post_layernorm\"]\n",
        "\n",
        "    model.vision_model.encoder.load_state_dict(encoder_weights)\n",
        "    model.vision_model.head.load_state_dict(head_weights)\n",
        "    model.vision_model.post_layernorm.load_state_dict(post_layernorm_weights)\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivL_UMSZRJ8T"
      },
      "outputs": [],
      "source": [
        "siglip_vision_model: SiglipVisionModel\n",
        "device: str = \"cuda\"\n",
        "images: list[Image] = []\n",
        "image_embeddings: torch.Tensor = []\n",
        "text_embeddings: torch.Tensor = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embeddings(BaseModel):\n",
        "    pixel_values: list[list[float]]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUtTQQ8uR5tS",
        "outputId": "da030b27-9db6-44b4-e7fd-ab8c0b1fb9b5"
      },
      "outputs": [],
      "source": [
        "@asynccontextmanager\n",
        "async def lifespan(app: FastAPI):\n",
        "    global \\\n",
        "        siglip_vision_model, \\\n",
        "        device\n",
        "    siglip_vision_model = load_vit(r\"\") # Path to your SigLIP model weights\n",
        "    siglip_vision_model.eval()\n",
        "\n",
        "    print(\"Ready for incoming...\")\n",
        "    yield\n",
        "\n",
        "app = FastAPI(title=\"SigLIP Image Search API\", lifespan=lifespan)\n",
        "\n",
        "# Enable CORS for React frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Get the service status\"\"\"\n",
        "    return {\"status\": \"OK\",\n",
        "            \"cuda\": torch.cuda.is_available()}\n",
        "\n",
        "\n",
        "@app.post(\"/generate-embeddings/\")\n",
        "async def generate_embeddings(embeddings_request: Embeddings):\n",
        "    \"\"\"Generate embeddings for the uploaded images\"\"\"\n",
        "    global image_embeddings\n",
        "\n",
        "    embeddings=[]\n",
        "    for i in range(len(embeddings_request.pixel_values)):\n",
        "        p = embeddings_request.pixel_values[i]\n",
        "        p = np.array(p, dtype=np.float32).reshape((196,768))\n",
        "        embeddings.append(p)\n",
        "    embeddings = np.stack(embeddings, axis=0)\n",
        "    embeddings = torch.tensor(embeddings).to('cuda')\n",
        "    \n",
        "    print(f\"Generating image embeddings from received embeddings of shape {embeddings.shape}...\")\n",
        "    with torch.no_grad():\n",
        "        image_embeddings = siglip_vision_model(embeddings)\n",
        "    image_embeddings = image_embeddings[\"pooler_output\"]\n",
        "    image_embeddings = torch.nn.functional.normalize(image_embeddings, p=2, dim=-1)\n",
        "    print(f\"Embeddings generated: {image_embeddings.shape}\")\n",
        "    print()\n",
        "\n",
        "    image_embeddings = image_embeddings.cpu().detach().numpy()\n",
        "    batch_size = image_embeddings.shape[0]\n",
        "    image_embeddings=image_embeddings.tolist()\n",
        "\n",
        "    return {\"image_embeddings\": image_embeddings, \"batch_size\": batch_size}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    auth_token = \"\"  # Replace with your ngrok auth token\n",
        "    if not auth_token:\n",
        "        raise ValueError(\"Please set your ngrok auth token.\")\n",
        "    \n",
        "    ngrok.set_auth_token(auth_token)\n",
        "    if len(ngrok.get_tunnels()) == 0:\n",
        "        ngrok_tunnel = ngrok.connect(8000)\n",
        "        public_url = ngrok_tunnel.public_url\n",
        "    else:\n",
        "        public_url = ngrok.get_tunnels()[0].public_url\n",
        "\n",
        "    print('Public URL:', public_url)\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    uvicorn.run(app, port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dhanispix",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
